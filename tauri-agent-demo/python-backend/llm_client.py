from typing import Optional, List, Dict, Any
import httpx
from models import LLMConfig, LLMApiType

class LLMClient:
    """ç»Ÿä¸€çš„ LLM å®¢æˆ·ç«¯ï¼Œæ”¯æŒå¤šç§ API"""
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self.timeout = 60.0
    
    async def chat(self, messages: List[Dict[str, str]]) -> Dict[str, Any]:
        """
        å‘é€èŠå¤©è¯·æ±‚åˆ° LLM
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [{"role": "user", "content": "..."}]
        
        Returns:
            åŒ…å«å®Œæ•´å“åº”çš„å­—å…¸ï¼ŒåŒ…æ‹¬ content å’ŒåŽŸå§‹å“åº”æ•°æ®
            {
                "content": str,  # LLM çš„å›žå¤å†…å®¹
                "raw_response": dict  # å®Œæ•´çš„åŽŸå§‹å“åº”
            }
        """
        if self.config.api_type == "openai":
            return await self._chat_openai(messages)
        elif self.config.api_type == "zhipu":
            return await self._chat_zhipu(messages)
        elif self.config.api_type == "deepseek":
            return await self._chat_deepseek(messages)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ API ç±»åž‹: {self.config.api_type}")
    
    async def chat_stream(self, messages: List[Dict[str, str]]):
        """
        æµå¼å‘é€èŠå¤©è¯·æ±‚åˆ° LLM
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [{"role": "user", "content": "..."}]
        
        Yields:
            str: é€ä¸ªç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µï¼ˆchunkï¼‰
        """
        if self.config.api_type == "openai":
            async for chunk in self._chat_openai_stream(messages):
                yield chunk
        elif self.config.api_type == "zhipu":
            async for chunk in self._chat_zhipu_stream(messages):
                yield chunk
        elif self.config.api_type == "deepseek":
            async for chunk in self._chat_deepseek_stream(messages):
                yield chunk
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ API ç±»åž‹: {self.config.api_type}")
    
    async def _chat_openai(self, messages: List[Dict[str, str]]) -> Dict[str, Any]:
        """OpenAI API è°ƒç”¨"""
        base_url = self.config.base_url or "https://api.openai.com/v1"
        
        request_payload = {
            "model": self.config.model,
            "messages": messages,
            "temperature": self.config.temperature,
            "max_tokens": self.config.max_tokens
        }
        
        # ðŸ”¥ å¦‚æžœæ˜¯æŽ¨ç†æ¨¡åž‹ï¼ˆO1/GPT-5 ç³»åˆ—ï¼‰ï¼Œæ·»åŠ  reasoning å‚æ•°
        model_lower = self.config.model.lower()
        if "o1" in model_lower or "gpt-5" in model_lower:
            # æŽ¨ç†æ¨¡åž‹ä¸æ”¯æŒ temperature
            request_payload.pop("temperature", None)
            
            # ä½¿ç”¨æ–°çš„ reasoning å¯¹è±¡æ ¼å¼
            reasoning_effort = getattr(self.config, 'reasoning_effort', 'medium')
            reasoning_summary = getattr(self.config, 'reasoning_summary', 'detailed')
            
            request_payload["reasoning"] = {
                "effort": reasoning_effort,
                "summary": reasoning_summary
            }
            
            print(f"ðŸ§  [Reasoning Mode] effort={reasoning_effort}, summary={reasoning_summary}")
        
        async with httpx.AsyncClient(timeout=self.timeout) as client:
            response = await client.post(
                f"{base_url}/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.config.api_key}",
                    "Content-Type": "application/json"
                },
                json=request_payload
            )
            response.raise_for_status()
            data = response.json()
            
            # æå–æŽ¨ç† tokens ä¿¡æ¯ï¼ˆå¦‚æžœæœ‰ï¼‰
            usage = data.get("usage", {})
            reasoning_tokens = usage.get("reasoning_tokens", 0)
            
            if reasoning_tokens > 0:
                print(f"ðŸ§  [Reasoning Tokens] {reasoning_tokens} tokens used for reasoning")
            
            return {
                "content": data["choices"][0]["message"]["content"],
                "raw_response": data,
                "reasoning_tokens": reasoning_tokens
            }
    
    async def _chat_openai_stream(self, messages: List[Dict[str, str]]):
        """OpenAI API æµå¼è°ƒç”¨"""
        import json
        
        base_url = self.config.base_url or "https://api.openai.com/v1"
        
        request_payload = {
            "model": self.config.model,
            "messages": messages,
            "temperature": self.config.temperature,
            "max_tokens": self.config.max_tokens,
            "stream": True
        }
        
        # ðŸ”¥ å¦‚æžœæ˜¯æŽ¨ç†æ¨¡åž‹ï¼ˆO1/GPT-5 ç³»åˆ—ï¼‰ï¼Œæ·»åŠ  reasoning å‚æ•°
        model_lower = self.config.model.lower()
        if "o1" in model_lower or "gpt-5" in model_lower:
            # æŽ¨ç†æ¨¡åž‹ä¸æ”¯æŒ temperature
            request_payload.pop("temperature", None)
            
            # ä½¿ç”¨æ–°çš„ reasoning å¯¹è±¡æ ¼å¼
            reasoning_effort = getattr(self.config, 'reasoning_effort', 'medium')
            reasoning_summary = getattr(self.config, 'reasoning_summary', 'detailed')
            
            request_payload["reasoning"] = {
                "effort": reasoning_effort,
                "summary": reasoning_summary
            }
            
            print(f"ðŸ§  [Reasoning Mode Stream] effort={reasoning_effort}, summary={reasoning_summary}")
        
        async with httpx.AsyncClient(timeout=self.timeout) as client:
            async with client.stream(
                "POST",
                f"{base_url}/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.config.api_key}",
                    "Content-Type": "application/json"
                },
                json=request_payload
            ) as response:
                response.raise_for_status()
                async for line in response.aiter_lines():
                    if line.startswith("data: "):
                        data = line[6:]
                        if data == "[DONE]":
                            break
                        try:
                            chunk = json.loads(data)
                            delta = chunk["choices"][0]["delta"]
                            if "content" in delta:
                                yield delta["content"]
                        except (json.JSONDecodeError, KeyError):
                            continue
    
    async def _chat_zhipu(self, messages: List[Dict[str, str]]) -> Dict[str, Any]:
        """æ™ºè°± AI API è°ƒç”¨"""
        base_url = self.config.base_url or "https://open.bigmodel.cn/api/paas/v4"
        
        request_payload = {
            "model": self.config.model,
            "messages": messages,
            "temperature": self.config.temperature,
            "max_tokens": self.config.max_tokens
        }
        
        async with httpx.AsyncClient(timeout=self.timeout) as client:
            response = await client.post(
                f"{base_url}/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.config.api_key}",
                    "Content-Type": "application/json"
                },
                json=request_payload
            )
            response.raise_for_status()
            data = response.json()
            return {
                "content": data["choices"][0]["message"]["content"],
                "raw_response": data
            }
    
    async def _chat_zhipu_stream(self, messages: List[Dict[str, str]]):
        """æ™ºè°± AI API æµå¼è°ƒç”¨"""
        import json
        
        base_url = self.config.base_url or "https://open.bigmodel.cn/api/paas/v4"
        
        request_payload = {
            "model": self.config.model,
            "messages": messages,
            "temperature": self.config.temperature,
            "max_tokens": self.config.max_tokens,
            "stream": True
        }
        
        async with httpx.AsyncClient(timeout=self.timeout) as client:
            async with client.stream(
                "POST",
                f"{base_url}/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.config.api_key}",
                    "Content-Type": "application/json"
                },
                json=request_payload
            ) as response:
                response.raise_for_status()
                async for line in response.aiter_lines():
                    if line.startswith("data: "):
                        data = line[6:]
                        if data == "[DONE]":
                            break
                        try:
                            chunk = json.loads(data)
                            delta = chunk["choices"][0]["delta"]
                            if "content" in delta:
                                yield delta["content"]
                        except (json.JSONDecodeError, KeyError):
                            continue
    
    async def _chat_deepseek(self, messages: List[Dict[str, str]]) -> Dict[str, Any]:
        """Deepseek API è°ƒç”¨"""
        base_url = self.config.base_url or "https://api.deepseek.com/v1"
        
        request_payload = {
            "model": self.config.model,
            "messages": messages,
            "temperature": self.config.temperature,
            "max_tokens": self.config.max_tokens
        }
        
        # ðŸ”¥ å¦‚æžœæ˜¯ deepseek-reasoner æ¨¡åž‹ï¼Œä¸è¦è®¾ç½® temperature
        if "reasoner" in self.config.model.lower():
            request_payload.pop("temperature", None)
        
        async with httpx.AsyncClient(timeout=self.timeout) as client:
            response = await client.post(
                f"{base_url}/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.config.api_key}",
                    "Content-Type": "application/json"
                },
                json=request_payload
            )
            response.raise_for_status()
            data = response.json()
            
            # æå–å†…å®¹ï¼ˆå¯èƒ½åŒ…å« reasoning_contentï¼‰
            message = data["choices"][0]["message"]
            content = message.get("content", "")
            reasoning_content = message.get("reasoning_content", "")
            
            # å¦‚æžœæœ‰æŽ¨ç†å†…å®¹ï¼Œåˆå¹¶æ˜¾ç¤º
            if reasoning_content:
                full_content = f"[æŽ¨ç†è¿‡ç¨‹]\n{reasoning_content}\n\n[å›žç­”]\n{content}"
            else:
                full_content = content
            
            return {
                "content": full_content,
                "raw_response": data
            }
    
    async def _chat_deepseek_stream(self, messages: List[Dict[str, str]]):
        """DeepSeek API æµå¼è°ƒç”¨"""
        import json
        
        base_url = self.config.base_url or "https://api.deepseek.com/v1"
        
        request_payload = {
            "model": self.config.model,
            "messages": messages,
            "temperature": self.config.temperature,
            "max_tokens": self.config.max_tokens,
            "stream": True
        }
        
        async with httpx.AsyncClient(timeout=self.timeout) as client:
            async with client.stream(
                "POST",
                f"{base_url}/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.config.api_key}",
                    "Content-Type": "application/json"
                },
                json=request_payload
            ) as response:
                response.raise_for_status()
                async for line in response.aiter_lines():
                    if line.startswith("data: "):
                        data = line[6:]
                        if data == "[DONE]":
                            break
                        try:
                            chunk = json.loads(data)
                            delta = chunk["choices"][0]["delta"]
                            if "content" in delta:
                                yield delta["content"]
                        except (json.JSONDecodeError, KeyError):
                            continue

def create_llm_client(config: LLMConfig) -> LLMClient:
    """åˆ›å»º LLM å®¢æˆ·ç«¯å®žä¾‹"""
    return LLMClient(config)
