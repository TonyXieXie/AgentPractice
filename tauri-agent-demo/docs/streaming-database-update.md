# æµå¼è¾“å‡ºçš„æ•°æ®åº“æ›´æ–°ç­–ç•¥

## æ ¸å¿ƒé—®é¢˜

**æµå¼è¾“å‡ºæ—¶ï¼Œä½•æ—¶æ›´æ–°æ•°æ®åº“ï¼Ÿ**

---

## ç®€çŸ­å›ç­”

âœ… **æ˜¯çš„ï¼Œæ•°æ®åº“æ›´æ–°åº”è¯¥æ”¾åœ¨æµå¼è¾“å‡ºç»“æŸæ—¶ï¼**

**åŸå› ï¼š**
1. æµå¼è¿‡ç¨‹ä¸­ï¼Œå†…å®¹æ˜¯é€æ­¥ç”Ÿæˆçš„ï¼Œè¿˜æ²¡æœ‰å®Œæ•´çš„æ¶ˆæ¯
2. æ•°æ®åº“éœ€è¦å­˜å‚¨**å®Œæ•´çš„æ¶ˆæ¯å†…å®¹**
3. åªæœ‰æµå¼ç»“æŸåï¼Œæ‰çŸ¥é“å®Œæ•´çš„å›å¤æ˜¯ä»€ä¹ˆ

---

## è¯¦ç»†è§£æ

### éæµå¼ vs æµå¼çš„æ•°æ®åº“æ›´æ–°

#### éæµå¼ï¼ˆå½“å‰å®ç°ï¼‰

```python
@app.post("/chat")
async def chat(request: ChatRequest):
    # 1. ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
    user_msg = db.create_message(ChatMessageCreate(
        session_id=session.id,
        role="user",
        content=request.message
    ))
    
    # 2. è°ƒç”¨ LLMï¼ˆç­‰å¾…å®Œæ•´å“åº”ï¼‰
    llm_result = await llm_client.chat(messages)
    
    # 3. ç«‹å³ä¿å­˜åŠ©æ‰‹æ¶ˆæ¯ï¼ˆå› ä¸ºå·²ç»æœ‰å®Œæ•´å†…å®¹ï¼‰
    assistant_msg = db.create_message(ChatMessageCreate(
        session_id=session.id,
        role="assistant",
        content=llm_result["content"],  # âœ… å®Œæ•´å†…å®¹
        raw_request=raw_request_data,
        raw_response=raw_response_data
    ))
    
    return ChatResponse(...)
```

**æ—¶é—´çº¿ï¼š**
```
[ç”¨æˆ·æ¶ˆæ¯] â†’ [ä¿å­˜åˆ°DB] â†’ [è°ƒç”¨LLM] â†’ [ç­‰å¾…...] â†’ [æ”¶åˆ°å®Œæ•´å“åº”] â†’ [ä¿å­˜åˆ°DB] â†’ [è¿”å›å‰ç«¯]
                â†‘                                                      â†‘
            ç«‹å³ä¿å­˜                                                ç«‹å³ä¿å­˜
```

#### æµå¼è¾“å‡º

```python
@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    # 1. ä¿å­˜ç”¨æˆ·æ¶ˆæ¯ï¼ˆç«‹å³ï¼‰
    user_msg = db.create_message(ChatMessageCreate(
        session_id=session.id,
        role="user",
        content=request.message
    ))
    
    # 2. æµå¼è°ƒç”¨ LLM
    llm_client = create_llm_client(config)
    
    async def generate():
        full_response = ""  # ç”¨äºç´¯ç§¯å®Œæ•´å“åº”
        
        # 3. é€å—å‘é€ç»™å‰ç«¯
        async for chunk in llm_client.chat_stream(llm_messages):
            full_response += chunk  # ç´¯ç§¯
            # å‘é€ SSE
            yield f"data: {json.dumps({'content': chunk})}\n\n"
        
        # 4. â­ æµå¼ç»“æŸåï¼Œä¿å­˜å®Œæ•´æ¶ˆæ¯åˆ°æ•°æ®åº“
        assistant_msg = db.create_message(ChatMessageCreate(
            session_id=session.id,
            role="assistant",
            content=full_response,  # âœ… å®Œæ•´å†…å®¹
            raw_request=raw_request_data,
            raw_response={"content": full_response}  # å¯èƒ½æ²¡æœ‰å®Œæ•´çš„ raw_response
        ))
        
        # 5. å‘é€ç»“æŸä¿¡å·
        yield "data: [DONE]\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )
```

**æ—¶é—´çº¿ï¼š**
```
[ç”¨æˆ·æ¶ˆæ¯] â†’ [ä¿å­˜åˆ°DB] â†’ [è°ƒç”¨LLMæµå¼] â†’ [æ”¶åˆ°chunk1] â†’ [å‘é€å‰ç«¯]
                â†‘                            â†“
            ç«‹å³ä¿å­˜                      [æ”¶åˆ°chunk2] â†’ [å‘é€å‰ç«¯]
                                            â†“
                                        [æ”¶åˆ°chunk3] â†’ [å‘é€å‰ç«¯]
                                            â†“
                                        [æµå¼ç»“æŸ] â†’ [ä¿å­˜å®Œæ•´æ¶ˆæ¯åˆ°DB]
                                                           â†‘
                                                    â­ åœ¨è¿™é‡Œä¿å­˜ï¼
```

---

## æ ¸å¿ƒç­–ç•¥

### âœ… æ¨èæ–¹æ¡ˆï¼šæµå¼ç»“æŸåä¿å­˜

```python
async def generate():
    full_response = ""
    raw_response_chunks = []
    
    try:
        # æµå¼å‘é€è¿‡ç¨‹
        async for chunk in llm_client.chat_stream(messages):
            full_response += chunk
            raw_response_chunks.append(chunk)
            
            # å®æ—¶å‘é€ç»™å‰ç«¯
            yield f"data: {json.dumps({'content': chunk})}\n\n"
        
        # â­ æµå¼æˆåŠŸç»“æŸï¼Œä¿å­˜å®Œæ•´æ¶ˆæ¯
        db.create_message(ChatMessageCreate(
            session_id=session.id,
            role="assistant",
            content=full_response,  # å®Œæ•´å†…å®¹
            raw_request=raw_request_data,
            raw_response={
                "content": full_response,
                "chunks": raw_response_chunks,
                "timestamp": datetime.now().isoformat()
            }
        ))
        
        # é€šçŸ¥å‰ç«¯æµå¼ç»“æŸ
        yield "data: [DONE]\n\n"
        
    except Exception as e:
        # âš ï¸ æµå¼ä¸­æ–­ï¼Œä¹Ÿè¦ä¿å­˜éƒ¨åˆ†å†…å®¹
        db.create_message(ChatMessageCreate(
            session_id=session.id,
            role="assistant",
            content=full_response + f"\n\n[Error: {str(e)}]",
            metadata={"error": str(e), "partial": True}
        ))
        
        yield f"data: {json.dumps({'error': str(e)})}\n\n"
```

---

## å…³é”®è€ƒè™‘å› ç´ 

### 1. ä¸ºä»€ä¹ˆä¸èƒ½åœ¨æµå¼è¿‡ç¨‹ä¸­æ›´æ–°ï¼Ÿ

#### âŒ æ–¹æ¡ˆAï¼šæ¯ä¸ªchunkéƒ½æ›´æ–°æ•°æ®åº“

```python
async def generate():
    message_id = None
    
    async for chunk in llm_client.chat_stream(messages):
        if message_id is None:
            # ç¬¬ä¸€ä¸ªchunkï¼šåˆ›å»ºæ¶ˆæ¯
            msg = db.create_message(ChatMessageCreate(
                session_id=session.id,
                role="assistant",
                content=chunk  # åªæœ‰ç¬¬ä¸€ä¸ªchunk
            ))
            message_id = msg.id
        else:
            # åç»­chunkï¼šæ›´æ–°æ¶ˆæ¯
            db.update_message(message_id, append_content=chunk)
        
        yield f"data: {json.dumps({'content': chunk})}\n\n"
```

**é—®é¢˜ï¼š**
- âŒ **æ€§èƒ½é—®é¢˜**ï¼šæ¯ä¸ªchunkéƒ½å†™æ•°æ®åº“ï¼Œå¤§é‡I/Oæ“ä½œ
- âŒ **å¹¶å‘é—®é¢˜**ï¼šå¦‚æœå¤šä¸ªæµå¼è¯·æ±‚åŒæ—¶è¿›è¡Œï¼Œæ•°æ®åº“å‹åŠ›å¤§
- âŒ **ä¸€è‡´æ€§é—®é¢˜**ï¼šå¦‚æœä¸­é€”å´©æºƒï¼Œæ•°æ®åº“ä¸­æ˜¯ä¸å®Œæ•´çš„æ¶ˆæ¯

#### âŒ æ–¹æ¡ˆBï¼šå®šæœŸæ‰¹é‡æ›´æ–°

```python
async def generate():
    buffer = ""
    msg_id = None
    
    async for chunk in llm_client.chat_stream(messages):
        buffer += chunk
        
        # æ¯100ä¸ªå­—ç¬¦æ›´æ–°ä¸€æ¬¡
        if len(buffer) >= 100:
            if msg_id is None:
                msg = db.create_message(...)
                msg_id = msg.id
            else:
                db.update_message(msg_id, content=buffer)
            buffer = ""
        
        yield f"data: {json.dumps({'content': chunk})}\n\n"
```

**é—®é¢˜ï¼š**
- âŒ **å¤æ‚åº¦é«˜**ï¼šéœ€è¦ç®¡ç†ç¼“å†²åŒºå’Œæ›´æ–°é€»è¾‘
- âŒ **ä»æœ‰ä¸€è‡´æ€§é—®é¢˜**ï¼šä¸­é€”å´©æºƒæ—¶æ•°æ®å¯èƒ½ä¸å®Œæ•´
- âŒ **æ€§èƒ½æ²¡æœ‰æ˜æ˜¾æå‡**ï¼šä»ç„¶æœ‰å¤šæ¬¡æ•°æ®åº“å†™å…¥

### 2. âœ… ä¸ºä»€ä¹ˆæ¨èåœ¨ç»“æŸæ—¶ä¿å­˜ï¼Ÿ

**ä¼˜ç‚¹ï¼š**
- âœ… **ç®€å•æ¸…æ™°**ï¼šé€»è¾‘ç®€å•ï¼Œæ˜“äºç†è§£å’Œç»´æŠ¤
- âœ… **æ€§èƒ½æœ€ä¼˜**ï¼šåªæœ‰ä¸€æ¬¡æ•°æ®åº“å†™å…¥
- âœ… **æ•°æ®å®Œæ•´æ€§**ï¼šä¿è¯å­˜å‚¨çš„æ˜¯å®Œæ•´æ¶ˆæ¯
- âœ… **äº‹åŠ¡æ€§**ï¼šè¦ä¹ˆå…¨éƒ¨ä¿å­˜ï¼Œè¦ä¹ˆå…¨éƒ¨ä¸ä¿å­˜

**ç¼ºç‚¹ï¼š**
- âš ï¸ **æµå¼ä¸­æ–­æ—¶å¯èƒ½ä¸¢å¤±éƒ¨åˆ†å†…å®¹**ï¼ˆå¯ä»¥é€šè¿‡å¼‚å¸¸å¤„ç†ç¼“è§£ï¼‰

---

## å¼‚å¸¸å¤„ç†ç­–ç•¥

### åœºæ™¯ 1ï¼šæµå¼ä¸­é€”ä¸­æ–­

```python
async def generate():
    full_response = ""
    
    try:
        async for chunk in llm_client.chat_stream(messages):
            full_response += chunk
            yield f"data: {json.dumps({'content': chunk})}\n\n"
        
        # æ­£å¸¸ç»“æŸï¼Œä¿å­˜å®Œæ•´æ¶ˆæ¯
        db.create_message(ChatMessageCreate(
            session_id=session.id,
            role="assistant",
            content=full_response
        ))
        
    except Exception as e:
        # â­ å¼‚å¸¸æƒ…å†µï¼šä¿å­˜éƒ¨åˆ†å†…å®¹å¹¶æ ‡è®°
        db.create_message(ChatMessageCreate(
            session_id=session.id,
            role="assistant",
            content=full_response + "\n\n[æµå¼ä¸­æ–­]",
            metadata={
                "error": str(e),
                "partial": True,  # æ ‡è®°ä¸ºéƒ¨åˆ†å†…å®¹
                "timestamp": datetime.now().isoformat()
            }
        ))
        
        # é€šçŸ¥å‰ç«¯é”™è¯¯
        yield f"data: {json.dumps({'error': str(e)})}\n\n"
```

### åœºæ™¯ 2ï¼šå®¢æˆ·ç«¯æ–­å¼€è¿æ¥

```python
from starlette.requests import Request

@app.post("/chat/stream")
async def chat_stream(request: ChatRequest, http_request: Request):
    async def generate():
        full_response = ""
        
        try:
            async for chunk in llm_client.chat_stream(messages):
                # æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦æ–­å¼€
                if await http_request.is_disconnected():
                    print("Client disconnected!")
                    # ä»ç„¶ä¿å­˜å·²ç”Ÿæˆçš„éƒ¨åˆ†
                    db.create_message(ChatMessageCreate(
                        session_id=session.id,
                        role="assistant",
                        content=full_response,
                        metadata={"partial": True, "reason": "client_disconnected"}
                    ))
                    break
                
                full_response += chunk
                yield f"data: {json.dumps({'content': chunk})}\n\n"
            
            # æ­£å¸¸ç»“æŸ
            db.create_message(ChatMessageCreate(
                session_id=session.id,
                role="assistant",
                content=full_response
            ))
            
        except Exception as e:
            # å¼‚å¸¸å¤„ç†...
```

---

## å‰ç«¯å¤„ç†

### æµå¼æ¥æ”¶å’Œæ˜¾ç¤º

```typescript
const handleSendStream = async () => {
    const response = await fetch('/chat/stream', {
        method: 'POST',
        headers: {'Content-Type': 'application/json'},
        body: JSON.stringify({
            message: userMessage,
            session_id: currentSessionId
        })
    });
    
    const reader = response.body!.getReader();
    const decoder = new TextDecoder();
    
    // åˆ›å»ºä¸´æ—¶æ¶ˆæ¯ï¼ˆID ä½¿ç”¨ä¸´æ—¶å€¼ï¼‰
    let tempMessage: Message = {
        id: Date.now(),  // ä¸´æ—¶ID
        session_id: currentSessionId,
        role: 'assistant',
        content: '',     // åˆå§‹ä¸ºç©º
        timestamp: new Date().toISOString()
    };
    
    setMessages(prev => [...prev, tempMessage]);
    
    // é€å—æ¥æ”¶å¹¶æ›´æ–°
    while (true) {
        const {done, value} = await reader.read();
        if (done) break;
        
        const chunk = decoder.decode(value);
        const lines = chunk.split('\n');
        
        for (const line of lines) {
            if (line.startsWith('data: ')) {
                const data = line.slice(6);
                if (data === '[DONE]') {
                    // â­ æµå¼ç»“æŸï¼Œé‡æ–°åŠ è½½æ¶ˆæ¯ï¼ˆè·å–çœŸå®IDå’Œå®Œæ•´æ•°æ®ï¼‰
                    const updatedMessages = await getSessionMessages(currentSessionId);
                    setMessages(updatedMessages);
                    return;
                }
                
                const parsed = JSON.parse(data);
                if (parsed.content) {
                    // æ›´æ–°ä¸´æ—¶æ¶ˆæ¯å†…å®¹
                    tempMessage.content += parsed.content;
                    setMessages(prev => [
                        ...prev.slice(0, -1),
                        {...tempMessage}
                    ]);
                }
            }
        }
    }
};
```

---

## å®Œæ•´ç¤ºä¾‹ä»£ç 

### åç«¯ï¼ˆFastAPIï¼‰

```python
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from typing import AsyncGenerator
import json

@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """æµå¼èŠå¤©æ¥å£"""
    
    # 1. è·å–/åˆ›å»ºä¼šè¯
    if request.session_id:
        session = db.get_session(request.session_id)
    else:
        session = db.create_session(ChatSessionCreate(
            title="æ–°å¯¹è¯",
            config_id=request.config_id
        ))
    
    # 2. è·å–é…ç½®
    config = db.get_config(session.config_id)
    
    # 3. ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
    user_msg = db.create_message(ChatMessageCreate(
        session_id=session.id,
        role="user",
        content=request.message
    ))
    
    # 4. è·å–å†å²å¹¶æ„å»ºæ¶ˆæ¯
    history = db.get_session_messages(session.id, limit=20)
    history_for_llm = [
        {"role": msg.role, "content": msg.content}
        for msg in history[:-1]
    ]
    
    llm_messages = message_processor.build_messages_for_llm(
        user_message=request.message,
        history=history_for_llm,
        system_prompt="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚"
    )
    
    # 5. å‡†å¤‡è°ƒè¯•æ•°æ®
    raw_request_data = {
        "model": config.model,
        "messages": llm_messages,
        "temperature": config.temperature,
        "max_tokens": config.max_tokens,
        "stream": True
    }
    
    # 6. æµå¼ç”Ÿæˆå‡½æ•°
    async def generate() -> AsyncGenerator[str, None]:
        full_response = ""
        
        try:
            llm_client = create_llm_client(config)
            
            # æµå¼è°ƒç”¨ LLM
            async for chunk in llm_client.chat_stream(llm_messages):
                full_response += chunk
                # å‘é€ç»™å‰ç«¯
                yield f"data: {json.dumps({'content': chunk})}\n\n"
            
            # â­ æµå¼ç»“æŸï¼Œä¿å­˜å®Œæ•´æ¶ˆæ¯
            db.create_message(ChatMessageCreate(
                session_id=session.id,
                role="assistant",
                content=full_response,
                raw_request=raw_request_data,
                raw_response={
                    "content": full_response,
                    "model": config.model,
                    "finish_reason": "stop"
                }
            ))
            
            # å‘é€ç»“æŸä¿¡å·
            yield "data: [DONE]\n\n"
            
        except Exception as e:
            # å¼‚å¸¸å¤„ç†ï¼šä¿å­˜éƒ¨åˆ†å†…å®¹
            if full_response:
                db.create_message(ChatMessageCreate(
                    session_id=session.id,
                    role="assistant",
                    content=full_response + "\n\n[æµå¼ä¸­æ–­]",
                    metadata={
                        "error": str(e),
                        "partial": True
                    }
                ))
            
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )
```

---

## æœ€ä½³å®è·µæ€»ç»“

### âœ… æ¨èåšæ³•

1. **ç”¨æˆ·æ¶ˆæ¯**ï¼šç«‹å³ä¿å­˜ï¼ˆåœ¨æµå¼å¼€å§‹å‰ï¼‰
2. **åŠ©æ‰‹æ¶ˆæ¯**ï¼šæµå¼ç»“æŸåä¿å­˜å®Œæ•´å†…å®¹
3. **å¼‚å¸¸å¤„ç†**ï¼šå¦‚æœä¸­æ–­ï¼Œä¿å­˜éƒ¨åˆ†å†…å®¹å¹¶æ ‡è®°
4. **å‰ç«¯å¤„ç†**ï¼šæµå¼ç»“æŸåé‡æ–°åŠ è½½æ¶ˆæ¯è·å–çœŸå®ID

### âš ï¸ æ³¨æ„äº‹é¡¹

1. **ç´¯ç§¯å†…å®¹**ï¼šåœ¨æµå¼è¿‡ç¨‹ä¸­ç”¨å˜é‡ç´¯ç§¯å®Œæ•´å“åº”
2. **é”™è¯¯å¤„ç†**ï¼šå³ä½¿ä¸­æ–­ä¹Ÿè¦ä¿å­˜å·²ç”Ÿæˆçš„å†…å®¹
3. **å®¢æˆ·ç«¯æ–­å¼€**ï¼šæ£€æµ‹æ–­å¼€å¹¶ä¿å­˜éƒ¨åˆ†å†…å®¹
4. **æ€§èƒ½ä¼˜åŒ–**ï¼šé¿å…åœ¨æµå¼è¿‡ç¨‹ä¸­é¢‘ç¹å†™æ•°æ®åº“

### ğŸ“Š æ•°æ®å®Œæ•´æ€§

| æƒ…å†µ | æ•°æ®åº“çŠ¶æ€ | å¤„ç†æ–¹å¼ |
|-----|-----------|---------|
| æ­£å¸¸å®Œæˆ | âœ… å®Œæ•´æ¶ˆæ¯ | åœ¨ç»“æŸæ—¶ä¿å­˜ |
| å¼‚å¸¸ä¸­æ–­ | âš ï¸ éƒ¨åˆ†æ¶ˆæ¯ | åœ¨catchä¸­ä¿å­˜ï¼Œæ ‡è®°partial |
| å®¢æˆ·ç«¯æ–­å¼€ | âš ï¸ éƒ¨åˆ†æ¶ˆæ¯ | æ£€æµ‹æ–­å¼€ï¼Œä¿å­˜å·²ç”Ÿæˆå†…å®¹ |
| æœåŠ¡å™¨å´©æºƒ | âŒ æ— è®°å½• | æ— æ³•å¤„ç†ï¼ˆå¯è€ƒè™‘å®šæœŸcheckpointï¼‰ |

---

## æ€»ç»“

**å›ç­”ä½ çš„é—®é¢˜ï¼š**

âœ… **æ˜¯çš„ï¼Œæ•°æ®åº“æ›´æ–°åº”è¯¥æ”¾åœ¨æµå¼è¾“å‡ºç»“æŸæ—¶ï¼**

**åŸå› ï¼š**
1. åªæœ‰ç»“æŸæ—¶æ‰æœ‰å®Œæ•´å†…å®¹
2. é¿å…é¢‘ç¹æ•°æ®åº“å†™å…¥
3. ä¿è¯æ•°æ®å®Œæ•´æ€§
4. ç®€åŒ–é€»è¾‘ï¼Œæ˜“äºç»´æŠ¤

**å®ç°è¦ç‚¹ï¼š**
- åœ¨ç”Ÿæˆå™¨å‡½æ•°ä¸­ç´¯ç§¯å®Œæ•´å“åº”
- æµå¼æˆåŠŸç»“æŸåä¿å­˜åˆ°æ•°æ®åº“
- å¼‚å¸¸æƒ…å†µä¸‹ä¹Ÿè¦ä¿å­˜éƒ¨åˆ†å†…å®¹
- å‰ç«¯åœ¨æ”¶åˆ° `[DONE]` åé‡æ–°åŠ è½½æ¶ˆæ¯
